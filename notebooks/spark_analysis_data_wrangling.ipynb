{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Goal\n",
    "### Overall Goal\n",
    "See how useful Spark is to explore a large data set.\n",
    "__*PUDF_base_all_tab.txt*__ is 10 GB and 18 M observations on 260 features.\n",
    "\n",
    "### Specific to this notebook\n",
    "Generating a cleaner data set and doing additional analysis on:\n",
    "1. PROVIDER_NAME\n",
    "2. *ADMIT_WEEKDAY*\n",
    "3. *pat_age*\n",
    "4. ~~RACE~~\n",
    "5. *ETHNICITY*\n",
    "6. ~~FIRST_PAYMENT_SRC~~\n",
    "7. ~~SECONDARY_PAYMENT_SRC~~\n",
    "8. ADMITTING_DIAGNOSIS\n",
    "9. *TYPE_OF_ADMISSION*\n",
    "10. *SOURCE_OF_ADMISSION*\n",
    "11. ~~SEX_CODE~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*Justifying Columns to be Removed*__\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "<th>Column</th><th>Reason</th>\n",
    "</tr>\n",
    "<tr><td>SEX_CODE</td><td>Data seems invalid. Male or Female account for less than 40% of observations</td></tr>\n",
    "<tr><td>RACE</td><td>Over 70% of the race values are not in the dicitonary.</td></tr>\n",
    "<tr><td>FIRST_PAYMENT_SRC</td><td>Variable is not in dictionary and values don't match any related columns in dictionary.</td></tr>\n",
    "<tr><td>SECONDARY_PAYMENT_SRC</td><td>Variable is not in dictionary and values don't match any related columns in dictionary.</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*Handling Invalid Data*__\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "<th>Column</th><th>Comments</th>\n",
    "</tr>\n",
    "<tr><td>SOURCE_OF_ADMISSION</td><td>Though most values are not in dictionary. The valid ones like <strong>doctor's referal</strong> and <strong>ED</strong> seem useful. Will assume explicit and implicit invalid values are null</td></tr>\n",
    "<tr><td>ETHNICITY</td><td>Most of data is invalid. However, the valid values have to do with being latin or not. Decided to keep valid columns and assume all invalid are non-Latin</td></tr>\n",
    "<tr><td>TYPE_OF_ADMISSION</td><td>Most values are in dictionary. Will assume explicit and implicit invalid values are null</td></tr>\n",
    "<tr><td>ADMIT_WEEKDAY</td><td>Most values are in dictionary. Will assume explicit and implicit invalid values are null</td></tr>\n",
    "<tr><td>pat_age</td><td>Will remove a few non-numeric values</td></tr>\n",
    "<tr><td>LENGTH_OF_STAY</td><td>Most values are in dictionary. Will assume explicit and implicit invalid values are null</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from os import path, getcwd\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_distinct_number(df, col):\n",
    "    if df is None or col is None:\n",
    "        print(\"The dataframe or col is invalid\")\n",
    "        return 0\n",
    "    elif col not in df.columns:\n",
    "        print(\"The column '{}' is not in the dataframe.\" \\\n",
    "             .format(col))\n",
    "    else:\n",
    "        return df.groupBy(col).count().count()\n",
    "\n",
    "def get_topN_group(df, col, sort=True, highest_first=True):\n",
    "    if df is None or col is None:\n",
    "        print(\"The dataframe or col is invalid\")\n",
    "        return 0\n",
    "    elif col not in df.columns:\n",
    "        print(\"The column '{}' is not in the dataframe.\" \\\n",
    "             .format(col))\n",
    "    else:\n",
    "        if sort:\n",
    "            return df.groupBy(col).count().orderBy(\"count\", ascending = not highest_first)\n",
    "        else:\n",
    "            return df.groupBy(col).count()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "texas_df = spark.read.format('com.databricks.spark.csv') \\\n",
    "    .options(header='true', inferschema='true', delimiter='\\t')\\\n",
    "    .load(path.join(getcwd(), \"..\", \"data\", \"PUDF_base_all_tab.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Munging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
