{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Goal\n",
    "### Overall Goal\n",
    "See how useful Spark is to explore a large data set.\n",
    "__*PUDF_base_all_tab.txt*__ is 10 GB and 18 M observations on 260 features.\n",
    "\n",
    "### Specific to this notebook\n",
    "Generating a cleaner data set and doing additional analysis on:\n",
    "1. PROVIDER_NAME\n",
    "2. *ADMIT_WEEKDAY*\n",
    "3. *pat_age*\n",
    "4. ~~RACE~~\n",
    "5. *ETHNICITY*\n",
    "6. ~~FIRST_PAYMENT_SRC~~\n",
    "7. ~~SECONDARY_PAYMENT_SRC~~\n",
    "8. ADMITTING_DIAGNOSIS\n",
    "9. *TYPE_OF_ADMISSION*\n",
    "10. *SOURCE_OF_ADMISSION*\n",
    "11. ~~SEX_CODE~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__*Justifying Columns to be Removed*__\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "<th>Column</th><th>Reason</th>\n",
    "</tr>\n",
    "<tr><td>SEX_CODE</td><td>Data seems invalid. Male or Female account for less than 40% of observations</td></tr>\n",
    "<tr><td>RACE</td><td>Over 70% of the race values are not in the dicitonary.</td></tr>\n",
    "<tr><td>FIRST_PAYMENT_SRC</td><td>Variable is not in dictionary and values don't match any related columns in dictionary.</td></tr>\n",
    "<tr><td>SECONDARY_PAYMENT_SRC</td><td>Variable is not in dictionary and values don't match any related columns in dictionary.</td></tr>\n",
    "<tr><td>TYPE_OF_ADMISSION</td><td>Most of the values are NULL. A lot of the actually provided values are not in the dictionary.</td></tr>\n",
    "<tr><td>SOURCE_OF_ADMISSION</td><td>Most values are not in dictionary.</td></tr>\n",
    "<tr><td>ETHNICITY</td><td>Number of hispanic folks was very small. Suggests the data is flawed.</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__*Handling Invalid Data*__\n",
    "<table align=\"left\">\n",
    "<tr>\n",
    "<th>Column</th><th>Comments</th>\n",
    "</tr>\n",
    "<tr><td>pat_age</td><td>Loosely numeric,but actually represents age range classes. Will attempt to impute values of the invalid age classes. Valid classes have values that can be parsed to numeric. Invalid ones do not.</td></tr>\n",
    "<tr><td>LENGTH_OF_STAY</td><td>Will remove the few non-numeric values</td></tr>\n",
    "<tr><td>ADMIT_WEEKDAY</td><td>Most values are in dictionary. Will remove the few invalid values (few than 0.1%)</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from os import path, getcwd\n",
    "import sys\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_distinct_number(df, col):\n",
    "    if df is None or col is None:\n",
    "        print(\"The dataframe or col is invalid\")\n",
    "        return 0\n",
    "    elif col not in df.columns:\n",
    "        print(\"The column '{}' is not in the dataframe.\" \\\n",
    "             .format(col))\n",
    "    else:\n",
    "        return df.groupBy(col).count().count()\n",
    "\n",
    "def get_topN_group(df, col, sort=True, highest_first=True):\n",
    "    if df is None or col is None:\n",
    "        print(\"The dataframe or col is invalid\")\n",
    "        return 0\n",
    "    elif col not in df.columns:\n",
    "        print(\"The column '{}' is not in the dataframe.\" \\\n",
    "             .format(col))\n",
    "    else:\n",
    "        if sort:\n",
    "            return df.groupBy(col).count().orderBy(\"count\", ascending = not highest_first)\n",
    "        else:\n",
    "            return df.groupBy(col).count()   \n",
    "\n",
    "def is_integer(test_string):\n",
    "    try:\n",
    "        int(test_string)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "is_integer_udf = pyspark.sql.functions.udf(is_integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "texas_df = spark.read.format('com.databricks.spark.csv') \\\n",
    "    .options(header='true', inferschema='true', delimiter='\\t')\\\n",
    "    .load(path.join(getcwd(), \"..\", \"data\", \"PUDF_base_all_tab.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('LENGTH_OF_STAY', 'string')\n",
      "('PROVIDER_NAME', 'string')\n",
      "('ADMIT_WEEKDAY', 'string')\n",
      "('pat_age', 'string')\n",
      "('ETHNICITY', 'string')\n",
      "('ADMITTING_DIAGNOSIS', 'string')\n",
      "('TYPE_OF_ADMISSION', 'string')\n",
      "('SOURCE_OF_ADMISSION', 'string')\n"
     ]
    }
   ],
   "source": [
    "for name, dtype in texas_df.select(\n",
    "    \"LENGTH_OF_STAY\",\n",
    "    \"PROVIDER_NAME\",\n",
    "    \"ADMIT_WEEKDAY\",\n",
    "    \"pat_age\",\n",
    "    \"ADMITTING_DIAGNOSIS\"\n",
    ").dtypes:\n",
    "    print(name, dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Munging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "texas_df_selected_subset = texas_df.select(\n",
    "    \"LENGTH_OF_STAY\",\n",
    "    \"PROVIDER_NAME\",\n",
    "    \"ADMIT_WEEKDAY\",\n",
    "    \"pat_age\",\n",
    "    \"ADMITTING_DIAGNOSIS\"\n",
    ").filter(\n",
    "    (texas_df[\"LENGTH_OF_STAY\"] != '*') &\\\n",
    "    (texas_df[\"LENGTH_OF_STAY\"] != 'LENGTH_OF_STAY') &\\\n",
    "    (texas_df[\"pat_age\"] != '*') &\\\n",
    "    (texas_df[\"pat_age\"] != '**') &\\\n",
    "    (texas_df[\"pat_age\"] != 'ZZ') &\\\n",
    "    (texas_df[\"ADMIT_WEEKDAY\"] != 'null') &\\\n",
    "    (texas_df[\"ADMIT_WEEKDAY\"] != '*') &\\\n",
    "    (texas_df[\"ADMIT_WEEKDAY\"] != 'RACE') &\\\n",
    "    (texas_df[\"ADMIT_WEEKDAY\"] != 'ADMIT_WEEKDAY') \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### LOS & Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__*Convert Numeric Fields*__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "texas_df_selected_subset = texas_df_selected_subset.select(\n",
    "    texas_df_selected_subset.LENGTH_OF_STAY.cast('float'),\n",
    "    \"PROVIDER_NAME\",\n",
    "    \"ADMIT_WEEKDAY\",\n",
    "    \"pat_age\", # is a class 01-22, 01-26\n",
    "    \"ADMITTING_DIAGNOSIS\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u\"cannot resolve 'CASE WHEN is_integer(pat_age) THEN `pat_age` ELSE '-99' END' due to data type mismatch: WHEN expressions in CaseWhen should all be boolean type, but the 1th when expression's type is is_integer(pat_age#27);;\\n'Project [LENGTH_OF_STAY#2402, PROVIDER_NAME#2, ADMIT_WEEKDAY#25, pat_age#27, ADMITTING_DIAGNOSIS#70, CASE WHEN is_integer(pat_age#27) THEN pat_age#27 ELSE -99 END AS PAT_AGE_CLASS#4516]\\n+- Project [cast(LENGTH_OF_STAY#26 as float) AS LENGTH_OF_STAY#2402, PROVIDER_NAME#2, ADMIT_WEEKDAY#25, pat_age#27, ADMITTING_DIAGNOSIS#70]\\n   +- Filter ((((((((NOT (LENGTH_OF_STAY#26 = *) && NOT (LENGTH_OF_STAY#26 = LENGTH_OF_STAY)) && NOT (pat_age#27 = *)) && NOT (pat_age#27 = **)) && NOT (pat_age#27 = ZZ)) && NOT (ADMIT_WEEKDAY#25 = null)) && NOT (ADMIT_WEEKDAY#25 = *)) && NOT (ADMIT_WEEKDAY#25 = RACE)) && NOT (ADMIT_WEEKDAY#25 = ADMIT_WEEKDAY))\\n      +- Project [LENGTH_OF_STAY#26, PROVIDER_NAME#2, ADMIT_WEEKDAY#25, pat_age#27, ADMITTING_DIAGNOSIS#70]\\n         +- Relation[discharge_qtr#0,thcic_id#1,PROVIDER_NAME#2,FAC_TEACHING_IND#3,FAC_PSYCH_IND#4,FAC_REHAB_IND#5,FAC_ACUTE_CARE_IND#6,FAC_SNF_IND#7,FAC_LONG_TERM_AC_IND#8,FAC_OTHER_LTC_IND#9,FAC_PEDS_IND#10,SPEC_UNIT_1#11,SPEC_UNIT_2#12,SPEC_UNIT_3#13,SPEC_UNIT_4#14,SPEC_UNIT_5#15,ENCOUNTER_INDICATOR#16,SEX_CODE#17,TYPE_OF_ADMISSION#18,SOURCE_OF_ADMISSION#19,PAT_STATE#20,PAT_ZIP#21,PAT_COUNTRY#22,county#23,... 236 more fields] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-714c9aab481d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m texas_df_selected_subset =texas_df_selected_subset.withColumn(\"PAT_AGE_CLASS\",    when(pyspark.sql.functions.udf(is_integer)(texas_df_selected_subset[\"pat_age\"]),         texas_df_selected_subset[\"pat_age\"]).otherwise(\"-99\")\n\u001b[0m\u001b[0;32m      2\u001b[0m )\n",
      "\u001b[1;32mC:\\Applications\\spark-2.1.1-bin-hadoop2.6\\python\\pyspark\\sql\\dataframe.pyc\u001b[0m in \u001b[0;36mwithColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   1500\u001b[0m         \"\"\"\n\u001b[0;32m   1501\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"col should be Column\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1502\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1504\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Applications\\spark-2.1.1-bin-hadoop2.6\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Applications\\spark-2.1.1-bin-hadoop2.6\\python\\pyspark\\sql\\utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: u\"cannot resolve 'CASE WHEN is_integer(pat_age) THEN `pat_age` ELSE '-99' END' due to data type mismatch: WHEN expressions in CaseWhen should all be boolean type, but the 1th when expression's type is is_integer(pat_age#27);;\\n'Project [LENGTH_OF_STAY#2402, PROVIDER_NAME#2, ADMIT_WEEKDAY#25, pat_age#27, ADMITTING_DIAGNOSIS#70, CASE WHEN is_integer(pat_age#27) THEN pat_age#27 ELSE -99 END AS PAT_AGE_CLASS#4516]\\n+- Project [cast(LENGTH_OF_STAY#26 as float) AS LENGTH_OF_STAY#2402, PROVIDER_NAME#2, ADMIT_WEEKDAY#25, pat_age#27, ADMITTING_DIAGNOSIS#70]\\n   +- Filter ((((((((NOT (LENGTH_OF_STAY#26 = *) && NOT (LENGTH_OF_STAY#26 = LENGTH_OF_STAY)) && NOT (pat_age#27 = *)) && NOT (pat_age#27 = **)) && NOT (pat_age#27 = ZZ)) && NOT (ADMIT_WEEKDAY#25 = null)) && NOT (ADMIT_WEEKDAY#25 = *)) && NOT (ADMIT_WEEKDAY#25 = RACE)) && NOT (ADMIT_WEEKDAY#25 = ADMIT_WEEKDAY))\\n      +- Project [LENGTH_OF_STAY#26, PROVIDER_NAME#2, ADMIT_WEEKDAY#25, pat_age#27, ADMITTING_DIAGNOSIS#70]\\n         +- Relation[discharge_qtr#0,thcic_id#1,PROVIDER_NAME#2,FAC_TEACHING_IND#3,FAC_PSYCH_IND#4,FAC_REHAB_IND#5,FAC_ACUTE_CARE_IND#6,FAC_SNF_IND#7,FAC_LONG_TERM_AC_IND#8,FAC_OTHER_LTC_IND#9,FAC_PEDS_IND#10,SPEC_UNIT_1#11,SPEC_UNIT_2#12,SPEC_UNIT_3#13,SPEC_UNIT_4#14,SPEC_UNIT_5#15,ENCOUNTER_INDICATOR#16,SEX_CODE#17,TYPE_OF_ADMISSION#18,SOURCE_OF_ADMISSION#19,PAT_STATE#20,PAT_ZIP#21,PAT_COUNTRY#22,county#23,... 236 more fields] csv\\n\""
     ]
    }
   ],
   "source": [
    "texas_df_selected_subset =\\\n",
    "texas_df_selected_subset.withColumn(\"PAT_AGE_CLASS\",\\\n",
    "    when(pyspark.sql.functions.udf(is_integer)(texas_df_selected_subset[\"pat_age\"]),\\\n",
    "         texas_df_selected_subset[\"pat_age\"]).otherwise(\"-99\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__*Verify converstion*__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covarience between LOS & Age is: 5.54018762405\n",
      "1 loop, best of 1: 1min 3s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit -n1 -r1 \\\n",
    "print(\"Covarience between LOS & Age is: {}\".format(\\\n",
    "    texas_df_selected_subset.stat.cov(\"LENGTH_OF_STAY\", \"pat_age\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|    LENGTH_OF_STAY|           pat_age|\n",
      "+-------+------------------+------------------+\n",
      "|  count|          17649996|          17649996|\n",
      "|   mean|2.2979328720527756|11.600564067726559|\n",
      "| stddev| 9.559145399910616|4.8548219739708705|\n",
      "|    min|               1.0|                00|\n",
      "|    max|            9999.0|                WC|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texas_df_selected_subset.describe(\"LENGTH_OF_STAY\", \"pat_age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original total 18158133.\n",
      "processed total 17909964.\n",
      "original: 48.\n",
      "processed: 28\n"
     ]
    }
   ],
   "source": [
    "print(\"original total {}.\\nprocessed total {}.\".format(texas_df.count(), texas_df_selected_subset.count()))\n",
    "print(\"original: {}.\\nprocessed: {}\"\\\n",
    "      .format(get_distinct_number(texas_df, 'pat_age'), \n",
    "             get_distinct_number(texas_df_selected_subset, 'pat_age')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|pat_age|  count|\n",
      "+-------+-------+\n",
      "|     MA|4610783|\n",
      "|     MC|3310117|\n",
      "|     12|1703555|\n",
      "|     09|1196770|\n",
      "|     HM|1115220|\n",
      "|     CI| 995008|\n",
      "|     BL| 957798|\n",
      "|     16| 539789|\n",
      "|     15| 393975|\n",
      "|     00| 371497|\n",
      "|     11| 296231|\n",
      "|     **| 261029|\n",
      "|     13| 228088|\n",
      "|     18| 178597|\n",
      "|     17| 170855|\n",
      "|     CH| 161169|\n",
      "|     07| 157008|\n",
      "|     14| 155918|\n",
      "|     19| 155492|\n",
      "|     08| 155088|\n",
      "|     ZZ| 152283|\n",
      "|     10| 115089|\n",
      "|     20|  94173|\n",
      "|     MB|  69652|\n",
      "|     OF|  63743|\n",
      "|     23|  62185|\n",
      "|     WC|  59674|\n",
      "|     21|  57908|\n",
      "|     06|  51705|\n",
      "|     02|  50076|\n",
      "+-------+-------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_topN_group(texas_df, 'pat_age').show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Munging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*Admit weekday*__\n",
    "\n",
    "Should only have days 1-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+------------------+-------+\n",
      "|ADMIT_WEEKDAY|               avg|            stddev|  count|\n",
      "+-------------+------------------+------------------+-------+\n",
      "|            7| 4.894237577829325|22.265765444919495| 245730|\n",
      "|            3|2.6647023225901676| 10.59716782799657|2371275|\n",
      "|            5|1.7696207153633128| 8.765847092424828|3472669|\n",
      "|            6| 5.121894903833316|  7.74468810346812| 254662|\n",
      "|            1| 4.368173355152989|15.498527644625481| 630013|\n",
      "|            4|1.9790783037914266| 3.491743657288062|9926633|\n",
      "|            2| 4.258737486882755|29.653987444012046| 749014|\n",
      "+-------------+------------------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texas_df_selected_subset.groupBy(\"ADMIT_WEEKDAY\")\\\n",
    ".agg(\n",
    "    pyspark.sql.functions.mean(\"LENGTH_OF_STAY\").alias(\"avg\"),\n",
    "    pyspark.sql.functions.stddev(\"LENGTH_OF_STAY\").alias(\"stddev\"),\n",
    "    pyspark.sql.functions.count(\"LENGTH_OF_STAY\").alias(\"count\")\n",
    ")\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persisting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texas_df_selected_subset.toPandas().to_csv(\n",
    "    path.join(getcwd(), \"..\", \"data\", \"texas_df_selected_subsetelected_subset.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Test stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+\n",
      "|LENGTH_OF_STAY|pat_age|\n",
      "+--------------+-------+\n",
      "|          0002|     07|\n",
      "|          0005|     12|\n",
      "+--------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = ['LENGTH_OF_STAY', 'pat_age']\n",
    "texas_df.select(cols).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|ADMIT_WEEKDAY|LENGTH_OF_STAY|\n",
      "+-------------+--------------+\n",
      "|         BOTH|           2.0|\n",
      "|         BOTH|           2.0|\n",
      "|         BOTH|           2.0|\n",
      "|         BOTH|           2.0|\n",
      "|         BOTH|           2.0|\n",
      "+-------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texas_df_selected_subset\\\n",
    "    .filter(\\\n",
    "        (texas_df_selected_subset[\"ADMIT_WEEKDAY\"] == 'null')|\\\n",
    "        (texas_df_selected_subset[\"ADMIT_WEEKDAY\"] == '*'))\\\n",
    "    .withColumn(\"ADMIT_WEEKDAY\", \n",
    "        when(\\\n",
    "            (texas_df_selected_subset[\"ADMIT_WEEKDAY\"] == 'null')|\\\n",
    "            (texas_df_selected_subset[\"ADMIT_WEEKDAY\"] == '*'), 'BOTH'\\\n",
    "            )\\\n",
    "    )\\\n",
    "    .select('ADMIT_WEEKDAY', 'LENGTH_OF_STAY').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|ETHNICITY|\n",
      "+---------+\n",
      "|      one|\n",
      "|      two|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texas_df.select(\"ETHNICITY\").distinct()\\\n",
    "    .filter((texas_df[\"ETHNICITY\"] == '1') | (texas_df[\"ETHNICITY\"] == '2'))\\\n",
    "    .withColumn(\"ETHNICITY\", when(texas_df[\"ETHNICITY\"] == '1', 'one').otherwise(\"two\")).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LENGTH_OF_STAY',\n",
       " 'PROVIDER_NAME',\n",
       " 'ADMIT_WEEKDAY',\n",
       " 'pat_age',\n",
       " 'ADMITTING_DIAGNOSIS']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texas_df_selected_subset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LENGTH_OF_STAY', 'PROVIDER_NAME', 'ADMIT_WEEKDAY', 'pat_age', 'ADMITTING_DIAGNOSIS']\n",
      "New col:\n",
      "['LENGTH_OF_STAY', 'PROVIDER_NAME', 'ADMIT_WEEKDAY', 'pat_age', 'ADMITTING_DIAGNOSIS', 'new_col']\n"
     ]
    }
   ],
   "source": [
    "test_df = spark.createDataFrame(sc.emptyRDD(), texas_df_selected_subset.schema)\n",
    "print(test_df.columns)\n",
    "test_df = test_df.withColumn(\"new_col\", pyspark.sql.functions.lit(None))\n",
    "print(\"New col:\\n{}\".format(test_df.columns))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
